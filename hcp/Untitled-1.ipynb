{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1304f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "import os\n",
    "\n",
    "HCP_DIR = \"./hcp\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions'\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'runs': [5,6],   'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'runs': [7,8],   'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'runs': [9,10],  'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'runs': [11,12], 'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'runs': [13,14], 'cond':['math','story']},\n",
    "    'RELATIONAL' : {'runs': [15,16], 'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'runs': [17,18], 'cond':['mental','rnd']}\n",
    "}\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec87d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hcp_task.tgz...\n"
     ]
    }
   ],
   "source": [
    "import os, requests, tarfile\n",
    "\n",
    "fname = \"hcp_task.tgz\"\n",
    "url = \"https://osf.io/s4h8j/download/\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      print(f\"Downloading {fname}...\")\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "      print(f\"Download {fname} completed!\")\n",
    "\n",
    "fname_ex = \"hcp_task\"\n",
    "path_name = os.path.join(HCP_DIR, fname_ex)\n",
    "if not os.path.exists(path_name):\n",
    "  print(f\"Extracting {fname_ex}.tgz...\")\n",
    "  with tarfile.open(f\"{fname_ex}.tgz\") as fzip:\n",
    "    fzip.extractall(HCP_DIR)\n",
    "else:\n",
    "  print(f\"File {fname_ex}.tgz has already been extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2993dd10",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './hcp\\\\hcp_task\\\\regions.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m my_subj \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     66\u001b[0m my_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 68\u001b[0m regions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHCP_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhcp_task\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregions.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     69\u001b[0m region_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(name\u001b[38;5;241m=\u001b[39mregions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     70\u001b[0m                    network\u001b[38;5;241m=\u001b[39mregions[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     71\u001b[0m                    hemi\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(N_PARCELS\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeft\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(N_PARCELS\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     73\u001b[0m group_contrast \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hcp\\\\hcp_task\\\\regions.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import os\n",
    "\n",
    "def load_single_timeseries(subject, experiment, run, dir, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    experiment (str): Name of experiment\n",
    "    run (int): 0-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_run = EXPERIMENTS[experiment]['runs'][run]\n",
    "  bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(os.path.join(bold_path, bold_file))\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "def load_evs(subject, experiment, run, dir):\n",
    "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    experiment (str) : Name of experiment\n",
    "    run (int) : 0-based run index, across all tasks\n",
    "\n",
    "  Returns\n",
    "    evs (list of lists): A list of frames associated with each condition\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "  task_key = 'tfMRI_' + experiment + '_' + ['RL', 'LR'][run]\n",
    "  for cond in EXPERIMENTS[experiment]['cond']:\n",
    "    ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\",\n",
    "                           str(task_key), f\"{cond}.txt\")\n",
    "\n",
    "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "    # Determine when trial starts, rounded down\n",
    "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "    # Use trial duration to determine how many frames to include for trial\n",
    "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "    # Take the range of frames that correspond to this specific trial\n",
    "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "    frames_list.append(frames)\n",
    "\n",
    "  return frames_list\n",
    "\n",
    "def average_frames(data, evs, experiment, cond):\n",
    "    idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
    "    return np.mean(np.concatenate([np.mean(data[:, evs[idx][i]], axis=1, keepdims=True) for i in range(len(evs[idx]))], axis=-1), axis=1)\n",
    "\n",
    "my_exp = 'WM'\n",
    "my_subj = 0\n",
    "my_run = 1\n",
    "\n",
    "regions = np.load(os.path.join(HCP_DIR, \"hcp_task\", \"regions.npy\")).T\n",
    "region_info = dict(name=regions[0].tolist(),\n",
    "                   network=regions[1],\n",
    "                   hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2))\n",
    "\n",
    "group_contrast = 0\n",
    "for s in subjects:\n",
    "  for r in [0, 1]:\n",
    "    data = load_single_timeseries(subject=s, experiment=my_exp, run=r,\n",
    "                                  dir=os.path.join(HCP_DIR, \"hcp_task\"),\n",
    "                                  remove_mean=True)\n",
    "    evs = load_evs(subject=s, experiment=my_exp, run=r,\n",
    "                   dir=os.path.join(HCP_DIR, \"hcp_task\"))\n",
    "\n",
    "    two_bk_body_activity = average_frames(data, evs, my_exp, '2bk_body')\n",
    "    zero_bk_body_activity = average_frames(data, evs, my_exp, '0bk_body')\n",
    "\n",
    "    contrast = zero_bk_body_activity - two_bk_body_activity\n",
    "    group_contrast += contrast\n",
    "\n",
    "group_contrast /= (len(subjects)*2)  # remember: 2 sessions per subject\n",
    "\n",
    "df = pd.DataFrame({'contrast': group_contrast,\n",
    "                   'network': region_info['network'],\n",
    "                   'hemi': region_info['hemi']\n",
    "                   })\n",
    "# we will plot the left foot minus right foot contrast so we only need one plot\n",
    "plt.figure()\n",
    "sns.barplot(y='network', x='contrast', data=df, hue='hemi')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
